<?xml version="1.0" encoding="UTF-8" ?>





<chapter xml:id="probdist" xmlns:xi="http://www.w3.org/2001/XInclude">
	<title>Probability Distributions</title>
	<introduction>
		<p>

			</p><p>



</p>
</introduction>


	<exercises>
		<title>Practice Problems</title>


		<exercise>
			<title>Probability Concept Review</title>
			<idx><h>Probability Concept Review</h></idx>
			<statement>
				<p>
Provide the definition and/or formula for each of the following terms. Then give an "intuitive description" of what this concept tells us, and why we might care. An illustrating concrete example (say, about vertex degrees in a network) can be a better way  convey your understanding than simply speaking in generalities.

			<ol>
				<li>
					<p>
 A random variable <m>X : \Omega \rightarrow \Z_{\geq 0}</m>.
					</p>
				</li>
				<li>
					<p>
 The probability mass function (PMF) <m>p_X(t)</m>.
					</p>
				</li>
				<li>
					<p>
 The expected value <m>\E[X]</m>.
					</p>
				</li>
				<li>
					<p>
 The variance <m>\var[X]</m>.

					</p>
				</li>
			</ol>

<!--solution>
<p>
			<ol>

				<li>
					<p>
 A random variable <m>X : \Omega \rightarrow \Z_{\geq 0}</m> is a function on the set <m>\Omega</m> of outcomes of an experiment.  We pick a sample outcome and measure one of its property. For example, we could pick a vertex of a network and write down its degree.

			</p><p>

This is useful when I want to get a sense of the structure (or typical structure) of a network, or another system.

					</p>
				</li>
				<li>
					<p>
 For <m>t \in \Z_{\geq 0}</m>, the value <m>p_X(t)</m> is the probability that <m>X=t</m> when we perform our experiment. For example, the PMF of "measure the degree of a randomly chosen vertex" is
<me>
p(t) = \frac{\mbox{number of vertices of degree <m>t</m>}}{\mbox{total number of vertices}}
</me>

			</p><p>
The PMF gives me a sense of what outcomes are possible, and how likely they are compared to one another.

					</p>
				</li>
				<li>
					<p>
 The expected value is
<me>
\E[X] = \sum_{t=0}^{\infty} t \cdot p_X(t)
</me>
is weighted average of all possible outcomes, using the PMF <m>p_X(t)</m> as the weight for outcome <m>X=t</m>.

			</p><p>
If I repeat my experiment many, many times, then the average outcome value of my experiment will be a number close to the expected value. However, any one particular outcome might be very far away from the average value.

			</p><p>
For example, knowing the average degree could be helpful when thinking about degree centrality. I can assess whether a given vertex has large degree or small degree in the spectrum of observed values.

					</p>
				</li>
				<li>
					<p>
 The variance is
<me>
\var[X] = E[X^2] - E[X]^2.
</me>
This number tells me about the "spread" of the distribution around its average value. A small variance means that the PMF is concentrated around the average value, so most of my experiments will return a value that is close to the average one. A large variance means that the values are more spread out.

			</p><p>

Taking the square root gives you the standard deviation <m>\sigma = \sqrt{\var[X]}</m>. We can compare this value directly to the expected value, since they have the same units. This helps to get a sense of whether the variance is "big" or "small." Comparing standard deviation to the expected value helps to give us some sense of scale.

			</p><p>
For example, in a graph with small variance, most of the vertices have similar degrees. In a graph with high variance, we will have a much more diverse collection of vertex degrees. In particular, there will be some vertices with much smaller degrees than typical, or some vertices with much larger degree than typical, or both.

					</p>
				</li>
			</ol>
</p>
</solution-->




			</p><p>
Now that  these concepts are at your fingertips, let's explore
three distributions that will  appear frequently in our discussion of network models and dynamic network processes. In the questions that follow, take some time to reflect on the formulas (as well as the techniques that got you there). For each question:
		<ul>
			<li>
			<p>
 Perform the calculation
			</p>
			</li>
			<li>
			<p>
 Pause to think about what the formula means.
			</p>
			</li>
			<li>
			<p>
 Interpret what the answer would mean for large <m>n</m>; for different values of <m>p \in [0,1]</m>; for big or small <m>k</m>.

			</p>
			</li>
		</ul>



				</p>
			</statement>
		</exercise>
		<exercise>
			<title>Binomial Random Variable</title>
			<idx><h>Binomial Random Variable</h></idx>
			<statement>
				<p>
Flip a weighted coin <m>n</m> times, where <m>p</m> is the probability of obtaining heads, and <m>1-p</m> is the probability of obtaining tails. Define <m>X</m> to be the number of heads in the <m>n</m> tosses. This is the <em>binomial random variable with parameters <m>n</m> and <m>p</m></em>.

			<ol>
				<li>
					<p>
 Give a combinatorial argument for why the PMF of the binomial random variable is
<me>
p(k) = {n \choose k} p^k (1 - p)^{n-k}.
</me>

<!--solution>
<p>
We give a combinatorial recipe.
		<ul>
			<li>
			<p>
 Pick the <m>k</m> flips that will be <m>H</m>. There are <m>{n \choose k}</m> ways to do so.
			</p>
			</li>
			<li>
			<p>
 The probability that the chosen <m>k</m> flips all turn up <m>H</m> is <m>p^k</m>.
			</p>
			</li>
			<li>
			<p>
 The probability that the remaining <m>n-k</m> flips all turn up <m>T</m> is <m>(1-p)^{n-k}</m>.
			</p>
			</li>
		</ul>
Multiplying all these terms together gives the expression above.
</p>
</solution-->


					</p>
				</li>
				<li>
					<p>
 Use the binomial theorem
<me>
(x+y)^n = \sum_{k=0}^n {n \choose k} x^ky^{n-k}
</me>
to prove that <m>p(k)</m> is a PMF.


<!--solution>
<p>
First, we note that <m>0 \leq p(k) \leq 1</m> for <m>0 \leq k \leq n</m>. Next, we must prove that the probabilities all sum to one. This follows quickly from the binomial theorem with <m>x=p</m> and <m>y=1-p</m>:
<me>
\sum_{k=0}^n {n \choose k} p^k (1-p)^{n-k} = (p + (1-p))^n = 1^n = 1.
</me>
</p>
</solution-->

					</p>
				</li>
				<li>
					<p>
 Linearity of expectation applies to the sum of multiple random variables. In particular, an easy proof by induction shows that for any random variables <m>X_1, X_2, \ldots, X_n</m>, we have
<me>
\E[X_1 + X_2 + \cdots + X_n] = \E[X_1] + \E[X_2] + \cdots + \E[X_n]
</me>
Use this formula to show that
<me>
\E[X] = np.
</me>
You will have to split your random variable <m>X</m> into the sum of <m>n</m> random variables.
(Note: you can also prove this directly from part (a), but it is a bit tedious.)




<!--solution>
<p>

Let <m>X_i</m> be an "indicator function" for whether coin <m>i</m> is <m>H</m>. That is,
<me>
X_i = \left\{
\begin{array}{cc}
1 &amp; \mbox{if the } i\mbox{th coin is } H, \\
0 &amp;  \mbox{if the } i\mbox{th coin is } T. \\
\end{array}
\right.
</me>
Then
<me>
\E[X] = \E\left[ \sum_{i=1}^n X_i \right] = \sum_{i=1}^n \E[X_i] =  \sum_{i=1}^n p = np.
</me>


For comparison, here is a proof that does not use linearity of expectation. It isn't "hard" but it does take a bit of work. Our strategy will be to
factor out an <m>n</m> and then reindex the sum.
</p>
<md>
<mrow>
\E[X] &amp;= \sum_{k=0}^n k p(k)
\, = \,  \sum_{k=0}^n k {n \choose k} p^k (1-p)^{n-k}
</mrow>
<mrow>
&amp; =  \sum_{k=1}^n  \frac{n!}{(k-1)! (n-k)!} p^k (1-p)^{n-k}
</mrow>
<mrow>
&amp; =  np \sum_{k=1}^n  \frac{(n-1)!}{(k-1)! (n-k)!} p^{k-1} (1-p)^{n-k}
</mrow>
<mrow>
&amp; =  np \sum_{j=0}^{n-1}  \frac{(n-1)!}{j! (n-1-j)!} p^{j} (1-p)^{n-1-j}
</mrow>
<mrow>
&amp;= np \sum_{j=0}^{n-1}  {n-1 \choose j}  p^{j} (1-p)^{n-1-j}
</mrow>
<mrow>
&amp; =  np  (p + (1-p))^{n-1} \, = \,  np.
</mrow>
</md>
<p>
We use a couple of tricks here
		<ul>
			<li>
			<p>
 At the third equality, we replace the lower limit of <m>k=0</m> with <m>k=1</m>. This is because the <m>k=0</m> term equals 0, so we do not need to list it. <em>This is an important step!</em> Do not underestimate the need to properly account for the lowest/highest index.
			</p>
			</li>
			<li>
			<p>
 At the fifth equality, we reindex, using <m>j</m> instead of <m>k</m>. <em>Do this carefully!</em> In my margin, I always write down the substitution in two ways. Here we are using <m>j=k-1</m>, which is the same as <m>k=j+1</m>. Having both of these expression staring at me will ensure that I don't make mistakes during the reindexing. Wherever I see a "<m>k</m>," I replace it with a "<m>j+1</m>." Or if I want to be fancy, I can immediately replace a "<m>k-1</m>" with a "<m>j</m>."
			</p>
			</li>
		</ul>
Don't cut corners on these substitutions. You will save yourself a lot of headaches!
</p>
</solution-->


					</p>
				</li>
				<li>
					<p>
 <em>[Do not solve this one. It is a long calculation.]</em> It can be shown that
<me>
\var(X) = np(1-p).
</me>
But you do not need to do this here. You need to use the fact that
<me>
\E[X^2] = \E[X(X-1) + X] = \E[X(X-1)] + \E[X],
</me>
which follows from <em>linearity of expectation</em>.

<!--solution>
<p>
This plays out much like "tedious calculation" in the answer to part (c), but it is more involved. We want to calculate
</p>
<md>
<mrow>
\E[X^2] - (\E[X])^2 &amp;= \E[X(X-1) + X] - (\E[X])^2
</mrow>
<mrow>
&amp;= \E[X(X-1)] + \E[X] - (\E[X])^2
</mrow>
</md>
<p>
Of these three terms, we already know that <m>\E[X]=np</m> and that <m>(E[X])^2 =  (np)^2</m>. So we only need to calculate <m>\E[X(X-1)] </m>. Here we go. This time, we will reindex using <m>j=k-2</m>, or equivalently, <m>k=j+2</m>.

</p>
<md>
<mrow>
\E[X(X-1)] &amp;= \sum_{k=0}^n k(k-1) p(k) \, = \, \sum_{k=0}^n  k(k-1) {n \choose k} p^k (1-p)^{n-k}
</mrow>
<mrow>
&amp; =  \sum_{k=2}^n  \frac{n!}{(k-2)! (n-k)!} p^k (1-p)^{n-k}
</mrow>
<mrow>
&amp;=  n(n-1)p^2 \sum_{k=2}^n  \frac{(n-2)!}{(k-1)! (n-k)!} p^{k-2} (1-p)^{n-k}
</mrow>
<mrow>
&amp; =  n(n-1)p^2  \sum_{j=0}^{n-2}  \frac{(n-2)!}{j! (n-2-j)!} p^{j} (1-p)^{n-2-j}
</mrow>
<mrow>
&amp;= n(n-1)p^2 \sum_{j=0}^{n-2}  {n-2 \choose j}  p^{j} (1-p)^{n-2-j}
</mrow>
<mrow>
&amp; =  n(n-1)p^2  (p + (1-p))^{n-2} \, = \,  n(n-1)p^2.
</mrow>
</md>
<p>
We are now ready to put it together:
</p>
<md>
<mrow>
\E[X^2] - (\E[X])^2 &amp;= n(n-1)p^2 + np -(np)^2
</mrow>
<mrow>
&amp;= (np)^2 - np^2 + np - (np)^2
</mrow>
<mrow>
&amp;= np(1-p).
</mrow>
</md>
<p>
</p>
</solution-->

					</p>
				</li>
			</ol>




				</p>
			</statement>
		</exercise>
		<exercise>
			<title>Geometric Random Variable</title>
			<idx><h>Geometric Random Variable</h></idx>
			<statement>
				<p>
Instead of flipping our coin for a fixed number of times, we will flip it until we first encounter a heads. This experiment gives the geometric random variable <m>Y</m>.



			<ol>
				<li>
					<p>
 Give a combinatorial argument for  why the PMF of the geometric random variable is
<me>
p(k) =  (1 - p)^{k-1}  p.
</me>

<!--solution>
<p>
In order for the <m>k</m>th flip to be the first <m>H</m> that we encounter, we must obtain <m>k-1</m> <m>T</m>'s in a row, followed by a <m>H</m>. The probability of this event is <m>(1-p)^{k-1}p</m>. Note that <m>p(0) = 0</m> since we cannot stop until we have seen a <m>H</m>.
</p>
</solution-->

					</p>
				</li>
				<li>
					<p>
 Use the fact that
<me>
\sum_{k=0}^{\infty} x^k = \frac{1}{1-x}
</me>
to prove that  <m>p(k)</m> is a PMF.

<!--solution>
<p>
It is easy to see that <m>0 \leq p(k) \leq 1</m> for all <m>k \in \Z^+</m>. Now we must show that the probabilities sum to 1. We have
</p>
<md>
<mrow>
\sum_{k=1}^{\infty} p(k) &amp;= \sum_{k=1}^{\infty} (1-p)^{k-1} p \, = \, p \sum_{k=1}^{\infty} (1-p)^{k-1}
</mrow>
<mrow>
&amp;= p \sum_{j=0}^{\infty} (1-p)^{j} \, = \, \frac{p}{1 - (1-p)} \,=\, \frac{p}{p} \,=\, 1.
</mrow>
</md>
<p>

</p>
</solution-->

					</p>
				</li>
				<li>
					<p>
 Show that
<me>
\E[Y] = \frac{1}{p}.
</me>
Hint: take the derivative of both sides of the equality in part (b) to get a new identity.

<!--solution>
<p>
We have
<me>
\E[Y] = \sum_{k=1}^{\infty}  k p(k) = \sum_{k=1}^{\infty}  k(1-p)^{k-1} p = p \sum_{k=1}^{\infty}  k(1-p)^{k-1}.
</me>
Taking the derivative of both sides of
<me>
\sum_{k=0}^{\infty} x^k = \frac{1}{1-x},
</me>
we find that
<me>
\sum_{k=1}^{\infty} k x^{k-1} = \frac{1}{(1-x)^2}.
</me>
Therefore
<me>
\E[Y] = p \sum_{k=1}^{\infty}  k(1-p)^{k-1} = \frac{p}{(1-(1-p))^2} = \frac{p}{p^2} = \frac{1}{p}.
</me>
</p>
</solution-->

					</p>
				</li>
				<li>
					<p>
 <em>[Do not solve this one. It is a long calculation.]</em>  It can be shown that
<me>
\var(Y) = \frac{1-p}{p^2}.
</me>
But you do not need to prove this here. (You use the same trick as in problem 1(d) of writing <m>\E[Y^2] = \E[Y(Y-1)] + E[Y]</m>.)


<!--solution>
<p>
We will calculate <m>\E[Y^2] - (\E[Y])^2 = \E[Y(Y-1)] + \E[Y] - (\E[Y])^2</m>.

</p>
<md>
<mrow>
\E[Y(Y-1)]  &amp;=&amp; \sum_{k=1}^{\infty}  k(k-1) (1-p)^{k-1} p
</mrow>
<mrow>
&amp;= p (1-p) \sum_{k=2}^{\infty}  k(k-1) (1-p)^{k-2}.
</mrow>
</md>
<p>
Taking the derivative of both sides of
<me>
\sum_{k=1}^{\infty} kx^{k-1} = \frac{1}{(1-x)^2},
</me>
we find that
<me>
\sum_{k=2}^{\infty} k(k-1) x^{k-2} = \frac{2}{(1-x)^3}.
</me>
Therefore
</p>
<md>
<mrow>
\E[Y(Y-1)]  &amp;=  p (1-p) \sum_{k=2}^{\infty}  k(k-1) (1-p)^{k-2}
</mrow>
<mrow>
&amp;= p (1-p) \frac{2}{(1- (1-p))^3} = \frac{p(1-p)}{p^3}
</mrow>
<mrow>
&amp; = \frac{p(1-p)}{p^2}.
</mrow>
</md>
<p>

</p>
</solution-->

					</p>
				</li>
			</ol>


				</p>
			</statement>
		</exercise>
		<exercise>
			<title>Poisson Random Variable</title>
			<idx><h>Poisson Random Variable</h></idx>
			<statement>
				<p>
The Poisson random variable <m>Z</m> for parameter <m>\lambda &gt; 0</m> has PMF given by
<me>
p(k) = e^{-\lambda} \frac{\lambda^k}{k!}.
</me>



			<ol>


				<li>
					<p>
 Use the fact that
<me>
\sum_{k=0}^{\infty} \frac{x^k}{k!} = e^{x}
</me>
to prove that  <m>p(k)</m> is a PMF.

<!--solution>
<p>
Once again, it is clear that <m>0 \leq p(k) </m> for all <m>k</m>. These numbers sum to
<me>
\sum_{k=0}^{\infty} e^{-\lambda} \frac{\lambda^k}{k!} = e^{-\lambda} \sum_{k=0}^{\infty}  \frac{\lambda^k}{k!}
= e^{-\lambda} e^{\lambda} =1.
</me>
Since all terms are nonnegative and they sum to 1, each term must be less than 1.
</p>
</solution-->

					</p>
				</li>
				<li>
					<p>
 Show that
<me>
\E[Z] = \lambda.
</me>

<!--solution>
<p>
</p>
<md>
<mrow>
\E[Z] &amp;= \sum_{k=0}^{\infty} k e^{-\lambda} \frac{\lambda^k}{k!}
\, = \,  \sum_{k=1}^{\infty} e^{-\lambda} \frac{\lambda^k}{(k-1)!}
</mrow>
<mrow>
&amp; =  \lambda \sum_{k=1}^{\infty} e^{-\lambda} \frac{\lambda^{k-1}}{(k-1)!}
\,= \,  \lambda \sum_{j=0}^{\infty} e^{-\lambda} \frac{\lambda^{j}}{j!}
</mrow>
<mrow>
&amp; = \lambda.
</mrow>
</md>
<p>
</p>
</solution-->

					</p>
				</li>
				<li>
					<p>
 <em>[Don't solve this one. The calculation isn't as bad as the other variance calculations above. But it is still a bit long.]</em> It can be shown that
<me>
\var[Z] = \lambda
</me>
as well! Again, we need to use the trick
<me>
\E[Z^2] = \E[Z(Z-1)] + \E[Z].
</me>
This time, we also need to be comfortable with reindexing infinite series.

<!--solution>
<p>
Here is the hard work:
</p>
<md>
<mrow>
\E[Z(Z-1)] &amp;= \sum_{k=0}^{\infty} k(k-1) e^{-\lambda} \frac{\lambda^k}{k!}
</mrow>
<mrow>
&amp;= \lambda^2 e^{-\lambda} \sum_{k=2}^{\infty}   \frac{\lambda^{k-2}}{(k-2)!}
</mrow>
<mrow>
&amp;= \lambda^2 e^{-\lambda} \sum_{j=0}^{\infty}   \frac{\lambda^{j}}{j!}
</mrow>
<mrow>
&amp;= \lambda^2 e^{-\lambda}e^{\lambda} = \lambda^2.
</mrow>
</md>
<p>
That wasn't so bad! Our variance calculation is
</p>
<md>
<mrow>
\var(Z) &amp;= \E[Z^2] - (\E[Z])^2 = \E[Z(Z-1)] + \E[Z]- (\E[Z])^2
</mrow>
<mrow>
&amp;= \lambda^2 + \lambda - \lambda^2 \, =\,  \lambda.
</mrow>
</md>
<p>
Fun fact: the expected value is also called the "first moment."
The variance is also called the "second moment." Can you guess what the third moment of the Poisson distribution is? That's right: it's also equal to <m>\lambda</m>. How about the fourth moment? Yep, <m>\lambda</m> again.

			</p><p>
In fact, every moment of the Poisson distribution is equal to <m>\lambda</m>. (This is analogous to how all of the derivatives of <m>e^x</m> are also <m>e^x</m>.) Each of the moments tell you something about the shape of the distribution. So in a poetic sense, the Poisson distribution must be the nicest-shaped bell curve centered at <m>x=\lambda</m>.

</p>
</solution-->


					</p>
				</li>
				<li>
					<p>
 In this part, you will show that when <m>n</m> is very large and <m>p</m> is very small, the Poisson PMF for
<me>
\lambda :=np
</me>
is a good approximation (for small <m>k</m>) of  the binomial PMF with parameters <m>n</m> and <m>p</m>.
We will fix <m>k \in \Z_{\geq 0}</m> to be  constant and let <m>n \rightarrow \infty</m>.
You will show that
<me>
{n \choose k} p^k (1-p)^{n-k} \approx e^{-\lambda} \frac{\lambda^k}{k!}   \mbox{if } k \ll n.
</me>
Use the following facts which hold for <m>n \rightarrow \infty</m> and any constants <m>c,d \in \Z_{\geq 0}</m>.
</p>
<md>
<mrow>
\lim_{n \rightarrow \infty} \frac{n -c}{n} &amp;= 1,
&amp;
\lim_{n \rightarrow \infty} \left( 1 - \frac{c}{n} \right)^{-d} &amp;= 1,
&amp;
\lim_{n \rightarrow \infty} \left( 1 - \frac{c}{n} \right)^n &amp;= e^{-c}.
</mrow>
</md>
<p>


<!--solution>
<p>
Keep in mind that <em><m>k</m> is a fixed number</em> while the size <m>n</m> of the network increases to infinity. This lets us use the three limits listed above.
</p>
<md>
<mrow>
{n \choose k} p^k (1-p)^{n-k}
&amp;= \frac{n!}{k! (n-k)!} \left( \frac{\lambda} {n}\right)^k  \left( 1 -  \frac{\lambda}{n} \right)^{n-k}
</mrow>
<mrow>
&amp;= \frac{\lambda^k} {k!} \left( \frac{n}{n} \frac{n-1}{n} \cdots \frac{n-k+1}{n} \right) \left( 1 -  \frac{\lambda}{n} \right)^{n} \left( 1 -  \frac{\lambda}{n} \right)^{-k}
</mrow>
<mrow>
&amp; \rightarrow
\frac{\lambda^k} {k!}  \cdot \big( 1 \cdot 1 \cdots 1  \big) \cdot  e^{-\lambda} \cdot 1 = e^{-\lambda} \frac{\lambda^k}{k!}
</mrow>
</md>
<p>
What this is saying is that as <m>n \rightarrow \infty</m>, the binomial distribution for <m>n</m> and <m>p</m> looks more and more like the Poisson distribution for <m>\lambda=np</m>.
</p>
</solution-->

					</p>
				</li>
			</ol>

				</p>
			</statement>
		</exercise>
	</exercises>


</chapter>
